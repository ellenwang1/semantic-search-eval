{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439a2d1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# POC semantic search E2E analysis\n",
    "\n",
    "The goal of this POC notebook is to import a model from hugging face, apply it to the dataset, evaluate on metrics, benchmark and create some visualisations. Essentially make sure the semantic search is working end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36657807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb759127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_examples = pd.read_parquet('../data/shopping_queries_dataset_examples.parquet')\n",
    "df_products = pd.read_parquet('../data/shopping_queries_dataset_products.parquet')\n",
    "df_sources = pd.read_csv(\"../data/shopping_queries_dataset_sources.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5eeb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/amazon-science/esci-data: suggested filter for task 1: Query-Product Ranking \n",
    "# Query-Product Ranking: Given a user specified query and a list of matched products, the goal of this \n",
    "# task is to rank the products so that the relevant products are ranked above the non-relevant ones.\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "df_task_1 = df_examples_products[df_examples_products[\"small_version\"] == 1]\n",
    "df_task_1_train = df_task_1[df_task_1[\"split\"] == \"train\"]\n",
    "df_task_1_test = df_task_1[df_task_1[\"split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39b94230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use code provided on huggingface to get started on multilingual text \n",
    "# semantic search siamese bert\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('SeyedAli/Multilingual-Text-Semantic-Search-Siamese-BERT-V1')\n",
    "\n",
    "df_task_1_train_light = df_task_1_train[df_task_1_train['query_id'].isin([1,2,3,4])]\n",
    "\n",
    "query_embeddings = model.encode(df_task_1_train_light['query'].tolist(), convert_to_tensor=True)\n",
    "example_embeddings = model.encode(df_task_1_train_light['product_title'].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d038835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ellen\\AppData\\Local\\Temp\\ipykernel_11768\\2807857632.py:2: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  similarities_mx = cosine_similarity(np.array(query_embeddings), np.array(example_embeddings))\n"
     ]
    }
   ],
   "source": [
    "# calculate cosine similarity and get diagonal\n",
    "similarities_mx = cosine_similarity(np.array(query_embeddings), np.array(example_embeddings))\n",
    "similarities_diag = np.diag(similarities_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acbb4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply esci mapping to esci label\n",
    "esci_weighting = {\n",
    "    'E': 3,\n",
    "    'S': 2,\n",
    "    'C': 1,\n",
    "    'I': 0\n",
    "}\n",
    "\n",
    "df_task_1_train_light['relevance'] = df_task_1_train_light['esci_label'].map(esci_weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cefabe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update index of subset df\n",
    "df_task_1_train_light = df_task_1_train_light.reset_index(drop=True)\n",
    "df_task_1_train_light.index = df_task_1_train_light.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbecc4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1\n",
      "0.50, example id 17\n",
      "0.49, example id 21\n",
      "0.45, example id 28\n",
      "0.44, example id 31\n",
      "0.42, example id 26\n",
      "Query ID: 3\n",
      "0.66, example id 103\n",
      "0.65, example id 89\n",
      "0.63, example id 90\n",
      "0.63, example id 88\n",
      "0.62, example id 93\n",
      "0.72157852690807\n"
     ]
    }
   ],
   "source": [
    "qrels_dict = {}\n",
    "run_dict = {}\n",
    "top_n = 5\n",
    "\n",
    "for query_id, group in df_task_1_train_light.groupby(\"query_id\"):\n",
    "    query_id_str = str(query_id)\n",
    "    # get actuals\n",
    "    qrels_dict[query_id_str] = {str(example): int(relevance) for example, relevance in zip(group[\"example_id\"], group[\"relevance\"])}\n",
    "    \n",
    "    # get scores paired to each example\n",
    "    examples = group[\"example_id\"].tolist()\n",
    "    example_score_pairs = list(zip(examples, similarities_diag[:len(examples)]))\n",
    "    \n",
    "    # filter for top_n examples per query\n",
    "    example_score_pairs_top_k = sorted(example_score_pairs, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # get predicted\n",
    "    run_dict[query_id_str] = {str(example): score for example, score in example_score_pairs}\n",
    "    \n",
    "    print(f\"Query ID: {query_id}\")\n",
    "    for example, score in example_score_pairs_top_k:\n",
    "        print(f\"{score:.2f}, example id {example}\")\n",
    "\n",
    "qrels = Qrels(qrels_dict)\n",
    "run = Run(run_dict)\n",
    "\n",
    "results = evaluate(qrels, run, metrics=[\"ndcg@10\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab19e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
